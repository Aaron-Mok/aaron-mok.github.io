<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blogs on </title>
    <link>https://aaron-mok.github.io/blog/</link>
    <description>Recent content in Blogs on </description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Sun, 17 Aug 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://aaron-mok.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Custom Camera Image Signal Processing (ISP) Pipeline with Raspberry Pi</title>
      <link>https://aaron-mok.github.io/blog/camera_isp/</link>
      <pubDate>Sun, 17 Aug 2025 00:00:00 +0000</pubDate>
      <guid>https://aaron-mok.github.io/blog/camera_isp/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://aaron-mok.github.io/img/blog/CameraISP/CoverArt_3.png&#34; alt=&#34;ISP art&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;This post presents a custom, real-time camera image signal processing (ISP) pipeline developed from scratch for the Raspberry Pi HQ camera, designed to meet the precise control requirements for future stereo vision applications. The pipeline encompasses raw Bayer capture, stride correction, black level subtraction, demosaicing, white balance, and color correction calibration, and gamma mapping to sRGB. A reusable Python library is provided to support others working with raw data from the Pi HQ camera (See &lt;a href=&#34;https://github.com/Aaron-Mok/Stereo-Camera/tree/main&#34;&gt;GitHub&lt;/a&gt;).&lt;/p&gt;</description>
    </item>
    <item>
      <title>My AI Piano Tutor</title>
      <link>https://aaron-mok.github.io/blog/pianokeydetection/</link>
      <pubDate>Sun, 06 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://aaron-mok.github.io/blog/pianokeydetection/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://aaron-mok.github.io/img/blog/AIPianoTutor/AITutorCoverArt.png&#34; alt=&#34;ToF art&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;I am a beginner piano player—despite holding ABRSM Grade 8 in both oboe and music theory. Piano presents a new set of challenges. For me, the difficulty lies in sight-reading: identifying notes on both clefs, finding the correct keys on the keyboard, and coordinating both hands in real time. That became the inspiration for this project: building my own AI-powered piano tutor using computer vision, a webcam, and a piano to improve my sight-reading.&lt;/p&gt;</description>
    </item>
    <item>
      <title>What is the best form lens? </title>
      <link>https://aaron-mok.github.io/blog/bestformlens/</link>
      <pubDate>Sat, 24 May 2025 00:00:00 +0000</pubDate>
      <guid>https://aaron-mok.github.io/blog/bestformlens/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://aaron-mok.github.io/img/blog/BestFormLens/bestformlens.png&#34; alt=&#34;best form lens art&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;I recently came across an interesting question: “What is the best form of a singlet lens?” At first, it seemed trivial — the answer is clearly the plano-convex lens. That’s what many optical design texts suggest, since it minimizes spherical aberration and coma. But I soon discovered that this answer relies on certain assumptions.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Light Detection and Ranging (LiDAR)</title>
      <link>https://aaron-mok.github.io/blog/lidar/</link>
      <pubDate>Sun, 12 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://aaron-mok.github.io/blog/lidar/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://aaron-mok.github.io/img/blog/LiDAR/LiDAR.webp&#34; alt=&#34;ToF art&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;In my last blog post, I discussed optical Time-of-Flight (ToF) sensing and explained how it works with a single emitter that produces a single beam of light and a single-pixel detector. Because of this setup, we could measure the ToF at only one point in space.&lt;/p&gt;&#xA;&lt;p&gt;The question, then, is: How can we sense depth across an entire area?&#xA;The answer lies in Light Detection and Ranging (LiDAR) — a system based on ToF that can measure depth over a broad region. In this blog post, I will (a) discuss the fundamentals of LiDAR and (b) demonstrate the optical system of a specific type of LiDAR, Flash LiDAR, using Zemax.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Optics tooling and mounts</title>
      <link>https://aaron-mok.github.io/blog/solidworks_showcase/</link>
      <pubDate>Wed, 08 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://aaron-mok.github.io/blog/solidworks_showcase/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://aaron-mok.github.io/img/blog/Toolings/tooling_coverart.webp&#34; alt=&#34;ToF art&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;This blog post showcases some optical tooling and mounts I designed in SolidWorks and Fusion.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Optical Time-of-Flight (ToF) sensing and lock-in detection</title>
      <link>https://aaron-mok.github.io/blog/time_of_flight/</link>
      <pubDate>Sat, 28 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://aaron-mok.github.io/blog/time_of_flight/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://aaron-mok.github.io/img/blog/ToF/time_of_flight.webp&#34; alt=&#34;ToF art&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;Distance measurements is important for a lot of applications, such as 3D imaging and depth sensing. One common method for distance measurement is by using optical time-of-flight. I will introduce two concepts: (a) The time-of-flight Principle and (b) lock-in detection/amplification to explain how it works.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Dual Excitation with Adaptive Excitation Polygon-scanning Multiphoton Microscope (DEEPscope)</title>
      <link>https://aaron-mok.github.io/blog/deepscope/</link>
      <pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://aaron-mok.github.io/blog/deepscope/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://aaron-mok.github.io/img/blog/Mouse3Drender.gif&#34; alt=&#34;DEEPscope Animation&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;In neuroscience research, imaging neuronal activity in the deep brain is crucial for understanding neurological diseases. However, as imaging depth increases, the field of view usually decreases exponentially. We have improved the fluorescence signal generation efficiency of three-photon microscopy, achieving an imaging field two order of magnitudes larger than traditional three-photon microscopes in deep brain regions with cellular resolution.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
